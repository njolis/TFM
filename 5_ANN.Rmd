---
title: 'Redes Neuronales Artificiales'
author: Escribir vuestro nombre y apellidos
subtitle: '`r params$subtitulo`'
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
# date: \today  (solo para pdf)
output:
  pdf_document:
    toc: yes
    df_print: kable
    highlight: zenburn
header-includes:
  - \usepackage[spanish]{babel}
params:
  p.train: !r 2/3
  seed.train: 12345
  seed.clsfier: 1234567
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE)
options(width=90)
```

```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("neuralnet", "NeuralNetTools", "ggplot2" ,"caret", "mice", "VIM", "gmodels", "ROSE", "dplyr")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```

\pagebreak

# Step 1 - Obtaining data

```{r}
dataset <- read.csv("data/data.Li21.csv", sep = ";")
```

## Step 2 - Exploraci?n y preparaci?n de los datos

En primer lugar se presenta los seis primeros registros:

```{r frag2, echo=FALSE}
#1
dataset<-dataset[complete.cases(dataset$outcome), ]
dataset<-dataset[,3:51]
#2
m.a.p<-(dataset$systolic.blood.pressure+2*dataset$diastolic.blood.pressure)/3
dataset<-cbind(dataset, m.a.p)
dataset<-dataset[,-c(15:16)]
#3
dataset$outcome<-factor(dataset$outcome, labels=c("Survivor", "Non-survivor"))
```

## Dataset A = data with no methodologies applied

```{r}
dataA<-dataset
dim(dataA) #Dimensions of the original dataset
round(mean(is.na(dataA))*100, 2)#% of the missing values
table(dataA$outcome)
```

## Dataset B = listwise deletion

```{r}
sum(complete.cases(dataA))#Determine the complete observations
nrow(dataA)-sum(complete.cases(dataA))#Determine the observations with missing values
dataB<-na.omit(dataA)#Create the dataset omitting the missing values 
sum(is.na(dataB))#Checking for missing values
dim(dataB)# Dimensions of the new dataset
table(dataB$outcome)
```

## Dataset C = KNN imputation

```{r}
k<- round(sqrt(nrow(dataA))) #Determine the best k
dataC<-kNN(dataA, variable = colnames(dataA), k = 34, imp_var = FALSE) #Generate the imputation with k=34
round(mean(is.na(dataC)*100), 2)#Checking for missing values
dim(dataC)# Dimensions of the new dataset
table(dataC$outcome)
```

## Dataset D = multiple imputation

```{r, warning=FALSE}
columns<- c("PCO2", "pH", "basophils", "lactic.acid", "BMI", "creatine.kinase", "lymphocyte", "neutrophils", "urine.output", "PT", "INR", "temperature", "glucose", 'm.a.p', "heart.rate", "respiratory.rate", "SP.O2", "blood.calcium") #Create a vector with the variables that need to be imputated.
pmm.data<- mice(dataA[,names(dataA) %in% columns], seed=12345, printFlag = FALSE, m = 30, method = "pmm")#Generate the imputation
imputed.data<- mice::complete(pmm.data)
complete.data<-dataA[, which((apply(is.na(dataA), 2, mean)*100)<0.01)]
dataD<-cbind(complete.data, imputed.data)#Create the new dataset
round(mean(is.na(dataD)*100), 2)#Checking for missing values
dim(dataD)# Dimensions of the new dataset
table(dataD$outcome)
```

### Transformaci?n de los datos num?ricos.

```{r frag5}
# custom normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}

data_nrmB <- as.data.frame(lapply(dataB[,-1],normalize))
data_nrmC <- as.data.frame(lapply(dataC[,-1],normalize))
data_nrmD <- as.data.frame(lapply(dataD[,-1],normalize))
```

### Creaci?n de variables binarias en lugar de usar la variable factor.

Ahora se crean tantas variables binarias como categorias tiene la variable `Class`.

```{r frag7}
# Creaci?n de variables binarias en lugar de usar la variable factor
data_nrmB$S <- dataB$outcome=="Survivor"
data_nrmB$N <- dataB$outcome=="Non-survivor"
data_nrmC$S <- dataC$outcome=="Survivor"
data_nrmC$N <- dataC$outcome=="Non-survivor"
data_nrmD$S <- dataD$outcome=="Survivor"
data_nrmD$N <- dataD$outcome=="Non-survivor"
```

### Partici?n de los datos en training/test

```{r}
#n_train <- 2/3
nB <- nrow(data_nrmB)
nC <- nrow(data_nrmC)
nD <- nrow(data_nrmD)
```

```{r}
# create training and test data
set.seed(params$seed.train)
#n_train <- 2/3
#n <- nrow(data_nrm)
trainB <- sample(nB,floor(nB*params$p.train))
data_nrmB.train <- data_nrmB[trainB,]
data_nrmB.test  <- data_nrmB[-trainB,]
```

```{r}
trainC <- sample(nC,floor(nC*params$p.train))
data_nrmC.train <- data_nrmC[trainC,]
data_nrmC.test  <- data_nrmC[-trainC,]
```

```{r}
trainD <- sample(nD,floor(nD*params$p.train))
data_nrmD.train <- data_nrmD[trainD,]
data_nrmD.test  <- data_nrmD[-trainD,]
```

# Step 3 - Entrenamiento del modelo.

## One node 

```{r, echo=FALSE,warning=FALSE,message=FALSE}
## Create a formula for a model with a large number of variables:
xnamB <- names(data_nrmB[1:47])
(fmlaB <- as.formula(paste("S+N ~ ", paste(xnamB, collapse= "+"))))
```

```{r,warning=FALSE,message=FALSE,fig.height=4  }

# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
data_model_B<- neuralnet(fmlaB,
                          data = data_nrmB.train,
                          hidden=1,linear.output=FALSE)

# visualize the network topology
plot(data_model_B, rep='best')

```

```{r, echo=FALSE,warning=FALSE,message=FALSE}
## Create a formula for a model with a large number of variables:
xnamC <- names(data_nrmC[1:47])
(fmlaC <- as.formula(paste("S+N ~ ", paste(xnamC, collapse= "+"))))
```

```{r,warning=FALSE,message=FALSE,fig.height=4  }

# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
data_model_C<- neuralnet(fmlaC,
                          data = data_nrmC.train,
                          hidden=1,linear.output=FALSE)

# visualize the network topology
plot(data_model_C, rep='best')

```

```{r, echo=FALSE,warning=FALSE,message=FALSE}
## Create a formula for a model with a large number of variables:
xnamD <- names(data_nrmD[1:47])
(fmlaD <- as.formula(paste("S+N ~ ", paste(xnamD, collapse= "+"))))
```

```{r,warning=FALSE,message=FALSE,fig.height=4  }

# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
data_model_D<- neuralnet(fmlaD,
                          data = data_nrmD.train,
                          hidden=1,linear.output=FALSE)

# visualize the network topology
plot(data_model_D, rep='best')
```

## Three nodes

```{r,warning=FALSE,message=FALSE,fig.height=4  }

# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
data_model_B3<- neuralnet(fmlaB,
                          data = data_nrmB.train,
                          hidden=3,linear.output=FALSE)

# visualize the network topology
plot(data_model_B3, rep='best')

```

```{r,warning=FALSE,message=FALSE,fig.height=4  }

# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
data_model_C3<- neuralnet(fmlaC,
                          data = data_nrmC.train,
                          hidden=3,linear.output=FALSE)

# visualize the network topology
plot(data_model_C3, rep='best')

```

```{r,warning=FALSE,message=FALSE,fig.height=4  }

# simple ANN with only a single hidden neuron
set.seed(params$seed.clsfier) # to guarantee repeatable results
data_model_D3<- neuralnet(fmlaD,
                          data = data_nrmD.train,
                          hidden=3,linear.output=FALSE)

# visualize the network topology
plot(data_model_D3, rep='best')
```

# Step 4 - Predicci?n y evaluaci?n del modelo

## One node

```{r,warning=FALSE,message=FALSE}
model_results_B <- neuralnet::compute(data_model_B, data_nrmB.test[,1:47])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx <- apply(model_results_B, 1, maxidx)
predictionB <- c('Survivor', 'Non-survivor')[idx]

prediction_factorB <- factor(predictionB, levels = c("Survivor", "Non-survivor"))

resB <- table(prediction_factorB, dataB$outcome[-trainB] )
table(predictionB)
table(dataB$outcome[-trainB])

# Results
require(caret)
cmatrixB <- confusionMatrix(resB,positive="Non-survivor")
cmatrixB
roc.curve(dataB$outcome[-trainB], prediction_factorB)
```

```{r,warning=FALSE,message=FALSE}
model_results_C <- neuralnet::compute(data_model_C, data_nrmC.test[,1:47])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idxC <- apply(model_results_C, 1, maxidx)
predictionC <- c('Survivor', 'Non-survivor')[idxC]

prediction_factorC <- factor(predictionC, levels = c("Survivor", "Non-survivor"))

resC <- table(prediction_factorC, dataC$outcome[-trainC] )
table(predictionC)
table(dataC$outcome[-trainC])

cmatrixC <- confusionMatrix(resC,positive="Non-survivor")
cmatrixC
roc.curve(dataC$outcome[-trainC], prediction_factorC)
```

```{r,warning=FALSE,message=FALSE}
model_results_D <- neuralnet::compute(data_model_D, data_nrmD.test[,1:47])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idxD <- apply(model_results_D, 1, maxidx)
predictionD <- c('Survivor', 'Non-survivor')[idxD]

prediction_factorD <- factor(predictionD, levels = c("Survivor", "Non-survivor"))

resD <- table(prediction_factorD, dataD$outcome[-trainD] )
table(predictionD)
table(dataD$outcome[-trainD])

cmatrixD <- confusionMatrix(resD,positive="Non-survivor")
cmatrixD
roc.curve(dataD$outcome[-trainD], prediction_factorD)
```

## Three nodes

```{r,warning=FALSE,message=FALSE}
model_results_B3 <- neuralnet::compute(data_model_B3, data_nrmB.test[,1:47])$net.result

# Put multiple binary output to categorical output
maxidx <- function(arr) {
  return(which(arr == max(arr)))
}

idx3 <- apply(model_results_B3, 1, maxidx)
predictionB3 <- c('Survivor', 'Non-survivor')[idx3]

prediction_factorB3 <- factor(predictionB3, levels = c("Survivor", "Non-survivor"))

resB3 <- table(prediction_factorB3, dataB$outcome[-trainB] )
table(predictionB3)
table(dataB$outcome[-trainB])

# Results
require(caret)
cmatrixB3 <- confusionMatrix(resB3,positive="Non-survivor")
cmatrixB3
roc.curve(dataB$outcome[-trainB], prediction_factorB3)
```

```{r,warning=FALSE,message=FALSE}
model_results_C3 <- neuralnet::compute(data_model_C3, data_nrmC.test[,1:47])$net.result

idxC3 <- apply(model_results_C3, 1, maxidx)
predictionC3 <- c('Survivor', 'Non-survivor')[idxC3]

prediction_factorC3 <- factor(predictionC3, levels = c("Survivor", "Non-survivor"))

resC3 <- table(prediction_factorC3, dataC$outcome[-trainC] )
table(predictionC3)
table(dataC$outcome[-trainC])

cmatrixC3 <- confusionMatrix(resC3,positive="Non-survivor")
cmatrixC3
roc.curve(dataC$outcome[-trainC], prediction_factorC3)
```

```{r,warning=FALSE,message=FALSE}
model_results_D3 <- neuralnet::compute(data_model_D3, data_nrmD.test[,1:47])$net.result

idxD3 <- apply(model_results_D3, 1, maxidx)
predictionD3 <- c('Survivor', 'Non-survivor')[idxD3]

prediction_factorD3 <- factor(predictionD3, levels = c("Survivor", "Non-survivor"))

resD3 <- table(prediction_factorD3, dataD$outcome[-trainD] )
table(predictionD3)
table(dataD$outcome[-trainD])

cmatrixD3 <- confusionMatrix(resD3,positive="Non-survivor")
cmatrixD3

roc.curve(dataD$outcome[-trainD], prediction_factorD3)
```

# Step 5 - Improving model performance

To improve model's performance a k-fold cross-validation method will be used. It consist in a resampling method that uses different portions of the data to test and train a model on different iterations. 

## One hidden node

### Dataset B

```{r}
data_nrmB.caret<- cbind(data_nrmB[, 1:47], outcome=dataB[,1])
data_nrmB.train.caret <- data_nrmB.caret[trainB,]
data_nrmB.test.caret  <- data_nrmB.caret[-trainB,]
```

```{r}
set.seed(params$seed.improv)
nnetGridD <-  expand.grid(size = 1,
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
model.kfold.annB <- train(form = outcome~ . , data = data_nrmB.train.caret, method='nnet', trControl= trainControl(method='cv'), 
                  tuneGrid=nnetGridD, trace = FALSE)
summary(model.kfold.annB)
plotnet(model.kfold.annB)
```

```{r}
predict.kfold.ann.B<-predict(model.kfold.annB, data_nrmB.test.caret)
resB.kfold<-table(predict.kfold.ann.B, data_nrmB.test.caret$outcome)
confusionMatrix(resB.kfold, positive= "Non-survivor")

roc.curve(data_nrmB.test.caret$outcome, predict.kfold.ann.B)
```

### Dataset C

```{r}
data_nrmC.caret<- cbind(data_nrmC[, 1:47], outcome=dataC[,1])
data_nrmC.train.caret <- data_nrmC.caret[trainC,]
data_nrmC.test.caret  <- data_nrmC.caret[-trainC,]
```

```{r}
set.seed(params$seed.improv)
nnetGridC <-  expand.grid(size = 1,
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
model.kfold.annC <- train(form = outcome~ . , data = data_nrmC.train.caret, method='nnet', trControl= trainControl(method='cv'), 
                  tuneGrid=nnetGridD, trace = FALSE)
summary(model.kfold.annC)
plotnet(model.kfold.annC)
```

```{r}
predict.kfold.ann.C<-predict(model.kfold.annC, data_nrmC.test.caret)
resC.kfold<-table(predict.kfold.ann.C, data_nrmC.test.caret$outcome)
confusionMatrix(resC.kfold, positive= "Non-survivor")

roc.curve(data_nrmC.test.caret$outcome, predict.kfold.ann.C)
```

### Dataset D

```{r}
data_nrmD.caret<- cbind(data_nrmD[, 1:47], outcome=dataD[,1])
data_nrmD.train.caret <- data_nrmD.caret[trainD,]
data_nrmD.test.caret  <- data_nrmD.caret[-trainD,]
```

```{r}
set.seed(params$seed.improv)
nnetGridD <-  expand.grid(size = 1,
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
model.kfold.annD <- train(form = outcome~ . , data = data_nrmD.train.caret, method='nnet', trControl= trainControl(method='cv'), 
                  tuneGrid=nnetGridD, trace = FALSE)
summary(model.kfold.annD)
plotnet(model.kfold.annD)
```

```{r}
predict.kfold.ann.D<-predict(model.kfold.annD, data_nrmD.test.caret)
resD.kfold<-table(predict.kfold.ann.D, data_nrmD.test.caret$outcome)
confusionMatrix(resD.kfold, positive= "Non-survivor")

roc.curve(data_nrmD.test.caret$outcome, predict.kfold.ann.D)
```


## Three hidden nodes

### Dataset B

```{r}
set.seed(params$seed.improv)
nnetGridD3 <-  expand.grid(size = 3,
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
model.kfold.annB3 <- train(form = outcome~ . , data = data_nrmB.train.caret, method='nnet', trControl= trainControl(method='cv'),
                  tuneGrid=nnetGridD3, trace = FALSE)
summary(model.kfold.annB3)
plotnet(model.kfold.annB3)
```

```{r}
predict.kfold.ann.B3<-predict(model.kfold.annB3, data_nrmB.test.caret)
resB3.kfold<-table(predict.kfold.ann.B3, data_nrmB.test.caret$outcome)
confusionMatrix(resB3.kfold, positive= "Non-survivor")

roc.curve(data_nrmB.test.caret$outcome, predict.kfold.ann.B3)
```

### Dataset C


```{r}
set.seed(params$seed.improv)
nnetGridC3 <-  expand.grid(size = 3,
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
model.kfold.annC3 <- train(form = outcome~ . , data = data_nrmC.train.caret, method='nnet', trControl= trainControl(method='cv'), 
                  tuneGrid=nnetGridD3, trace = FALSE)
summary(model.kfold.annC3)
plotnet(model.kfold.annC3)
```

```{r}
predict.kfold.ann.C3<-predict(model.kfold.annC3, data_nrmC.test.caret)
resC3.kfold<-table(predict.kfold.ann.C3, data_nrmC.test.caret$outcome)
confusionMatrix(resC3.kfold, positive= "Non-survivor")

roc.curve(data_nrmC.test.caret$outcome, predict.kfold.ann.C3)
```

### Dataset D

```{r}
set.seed(params$seed.improv)
nnetGridD3 <-  expand.grid(size = 3,
                        decay = seq(from = 0.1, to = 0.5, by = 0.1))
model.kfold.annD3 <- train(form = outcome~ . , data = data_nrmD.train.caret, method='nnet', trControl= trainControl(method='cv'), 
                  tuneGrid=nnetGridD3, trace = FALSE)
summary(model.kfold.annD3)
plotnet(model.kfold.annD3)
```

```{r}
predict.kfold.ann.D3<-predict(model.kfold.annD3, data_nrmD.test.caret)
resD3.kfold<-table(predict.kfold.ann.D3, data_nrmD.test.caret$outcome)
confusionMatrix(resD3.kfold, positive= "Non-survivor")

roc.curve(data_nrmD.test.caret$outcome, predict.kfold.ann.D3)
```



