---
title: 'Comparison of SVM models for outcome prediction of ICU-admitted heart failure patients'
author: NÃºria Jolis Orriols
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
output:
  pdf_document:
    toc: yes
    df_print: kable
header-includes:
  - \usepackage[english]{babel}
params:
  p.train: !r 0.7
  seed.train: 12345
  seed.clsfier: 1234567
  seed.improv: 123
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE, error = TRUE)
options(width=90)
```

```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("kernlab", "ggplot2" ,"caret", "tidyverse", "dplyr", "mice", "VIM", "gmodels", "ROSE")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```

# Step 1 - Collecting data

```{r}
dataA <- read.csv("data/datasetA.csv",stringsAsFactors=T)
dataB <- read.csv("data/datasetB.csv",stringsAsFactors=T)
dataC <- read.csv("data/datasetC.csv",stringsAsFactors=T)
dataD <- read.csv("data/datasetD.csv",stringsAsFactors=T)
```

# Step 2 - Model training 

## Data training/test partition into 70-30%

```{r}
#n_train <- 0.7
#Data A
set.seed(12345)
nA<-nrow(dataA)
trainA<-sample(nA,floor(nA*params$p.train))
dataA_train<-dataA[trainA,]
dataA_test<-dataA[-trainA,]
```

```{r}
#Data B
set.seed(12345)
nB<-nrow(dataA)
trainB<-sample(nB,floor(nB*params$p.train))
dataB_train<-dataB[trainB,]
dataB_test<-dataB[-trainB,]
```

```{r}
#Data C
nC<-nrow(dataC)
trainC<-sample(nC,floor(nC*params$p.train))
dataC_train<-dataC[trainC,]
dataC_test<-dataC[-trainC,]
```

```{r}
#Data D
set.seed(12345)
nD<-nrow(dataD)
trainD<-sample(nD,floor(nD*params$p.train))
dataD_train<-dataD[trainD,]
dataD_test<-dataD[-trainD,]
```

## Model training 

We are going to generate SVM models with the training data to predict the outcome. We use the ksvm() function of the kernlab package. We are going to try four different kernels: linear, Gaussian, Laplacian and hyperbolic.

### Linear

```{r,warning=FALSE,message=FALSE}
invisible(capture.output(modelA.svm.lin<- ksvm(outcome ~ ., data = dataA_train, kernel = "vanilladot")))
invisible(capture.output(modelB.svm.lin<- ksvm(outcome ~ ., data = dataB_train, kernel = "vanilladot")))
invisible(capture.output(modelC.svm.lin<- ksvm(outcome ~ ., data = dataC_train, kernel = "vanilladot")))
invisible(capture.output(modelD.svm.lin<- ksvm(outcome ~ ., data = dataD_train, kernel = "vanilladot")))
```

### Radial basis 

```{r,warning=FALSE,message=FALSE}
modelA.svm.rad<- ksvm(outcome ~ ., data = dataA_train, kernel = "rbfdot")
modelB.svm.rad<- ksvm(outcome ~ ., data = dataB_train, kernel = "rbfdot")
modelC.svm.rad<- ksvm(outcome ~ ., data = dataC_train, kernel = "rbfdot")
modelD.svm.rad<- ksvm(outcome ~ ., data = dataD_train, kernel = "rbfdot")
```

### Polynomial

```{r,warning=FALSE,message=FALSE}
invisible(capture.output(modelA.svm.pol<- ksvm(outcome ~ ., data = dataA_train, kernel = "polydot")))
invisible(capture.output(modelB.svm.pol<- ksvm(outcome ~ ., data = dataB_train, kernel = "polydot")))
invisible(capture.output(modelC.svm.pol<- ksvm(outcome ~ ., data = dataC_train, kernel = "polydot")))
invisible(capture.output(modelD.svm.pol<- ksvm(outcome ~ ., data = dataD_train, kernel = "polydot")))
```

# Step 3 - Model prediction, validation and evaluation

**Prediction**

We use the function `predict` to generate predictions on the testing dataset:

## Linear 

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
predict.modelA.lin <- predict(modelA.svm.lin, dataA_test)
predict.modelB.lin <- predict(modelB.svm.lin, dataB_test)
predict.modelC.lin <- predict(modelC.svm.lin, dataC_test)
predict.modelD.lin <- predict(modelD.svm.lin, dataD_test)
```


## Radial basis

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
predict.modelA.rad <- predict(modelA.svm.rad, dataA_test)
predict.modelB.rad <- predict(modelB.svm.rad, dataB_test)
predict.modelC.rad <- predict(modelC.svm.rad, dataC_test)
predict.modelD.rad <- predict(modelD.svm.rad, dataD_test)
```


## Polynomial

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
predict.modelA.pol <- predict(modelA.svm.pol, dataA_test)
predict.modelB.pol <- predict(modelB.svm.pol, dataB_test)
predict.modelC.pol <- predict(modelC.svm.pol, dataC_test)
predict.modelD.pol <- predict(modelD.svm.pol, dataD_test)
```


**Evaluation**

Evaluation of the classification models using a confusion matrix that compares the actual target values with those predicted by the machine learning model. 

## Linear 

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabA.svm.lin<-table(predict.modelA.lin, dataA_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabB.svm.lin<-table(predict.modelB.lin, dataB_test$outcome)
(cmatrix<-confusionMatrix(tabB.svm.lin, positive = "Non-survivor"))

roc.curve(dataB_test$outcome, predict.modelB.lin)

```


```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabC.svm.lin<-table(predict.modelC.lin, dataC_test$outcome)
(cmatrix<-confusionMatrix(tabC.svm.lin, positive = "Non-survivor"))

roc.curve(dataC_test$outcome, predict.modelC.lin)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabD.svm.lin<-table(predict.modelD.lin, dataD_test$outcome)
(cmatrix<-confusionMatrix(tabD.svm.lin, positive = "Non-survivor"))

roc.curve(dataD_test$outcome, predict.modelD.lin)
```

### Radial basis

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabA.svm.rad<-table(predict.modelA.rad, dataA_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabB.svm.rad<-table(predict.modelB.rad, dataB_test$outcome)
(cmatrix<-confusionMatrix(tabB.svm.rad, positive = "Non-survivor"))

roc.curve(dataB_test$outcome, predict.modelB.rad)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabC.svm.rad<-table(predict.modelC.rad, dataC_test$outcome)
(cmatrix<-confusionMatrix(tabC.svm.rad, positive = "Non-survivor"))

roc.curve(dataC_test$outcome, predict.modelC.rad)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabD.svm.rad<-table(predict.modelD.rad, dataD_test$outcome)
(cmatrix<-confusionMatrix(tabD.svm.rad, positive = "Non-survivor"))

roc.curve(dataD_test$outcome, predict.modelD.rad)
```

### Polynomial

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabA.svm.pol<-table(predict.modelA.pol, dataA_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabB.svm.pol<-table(predict.modelB.pol, dataB_test$outcome)
(cmatrix<-confusionMatrix(tabB.svm.pol, positive = "Non-survivor"))

roc.curve(dataB_test$outcome, predict.modelB.pol)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabC.svm.pol<-table(predict.modelC.pol, dataC_test$outcome)
(cmatrix<-confusionMatrix(tabC.svm.pol, positive = "Non-survivor"))

roc.curve(dataC_test$outcome, predict.modelC.pol)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabD.svm.pol<-table(predict.modelD.pol, dataD_test$outcome)
(cmatrix<-confusionMatrix(tabD.svm.pol, positive = "Non-survivor"))

roc.curve(dataD_test$outcome, predict.modelD.pol)
```


Of the tested models, the linear SVMs with datasets D and E are the best. The prediction of linear and polynomial models is identical, so the simplest of the two algorithms (the linear) is selected. The difference between the predictions with datasets D and E is practically negligible; therefore, an attempt to improve them both will be made.
The models with the datasets A and C did not converge because the datasets contain missing values. 

# Step 4 - Model's validation

To improve model's performance a k-fold cross-validation method will be used. It consist in a resampling method that uses different portions of the data to test and train a model on different iterations. 

This method will be performed on SVM linear of dataset C and D:

### Dataset C 

```{r}
set.seed(params$seed.improv)
model.kfold.C <- train(form = outcome~ . , data = dataC_train, method='svmLinear',trControl= trainControl(method='cv'), 
                  tuneGrid=NULL, trace = FALSE)
model.kfold.C
```

```{r}
predict.kfold.C<-predict(model.kfold.C, dataC_test)
resC<-table(predict.kfold.C, dataC_test$outcome)
confusionMatrix(resC, positive= "Non-survivor")

roc.curve(dataC_test$outcome, predict.kfold.C)
```

### Dataset D 

```{r}
set.seed(params$seed.improv)
model.kfold.D <- train(form = outcome~ . , data = dataD_train, method='svmLinear',trControl= trainControl(method='cv'), 
                  tuneGrid=NULL, trace = FALSE)
model.kfold.D
```

```{r}
predict.kfold.D<-predict(model.kfold.D, dataD_test)
resD<-table(predict.kfold.D, dataD_test$outcome)
confusionMatrix(resD, positive= "Non-survivor")

roc.curve(dataD_test$outcome, predict.kfold.D)
```

The results obtained are exactly the same. Therefore, in this case, the method of k-fold cross-validation did not improve our model's performance. 


