---
title: 'Comparison of SVM models for outcome prediction of ICU-admitted heart failure patients'
author: NÃºria Jolis Orriols
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
output:
  pdf_document:
    toc: yes
    df_print: kable
  html_document:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
  html_notebook:
    df_print: paged
    toc: yes
    toc_float: true
    theme: united
    highlight: tango
nocite: |
  @lantz2015machine
header-includes:
  - \usepackage[english]{babel}
params:
  file1: data01.csv
  folder.data: TFM
  p.train: !r 0.7
  seed.train: 12345
  seed.clsfier: 1234567
  seed.improv: 123
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NULL, cache=TRUE, error = TRUE)
options(width=90)
```

```{r packages, message=FALSE, echo=FALSE, warning=FALSE}
libraries <- c("kernlab", "ggplot2" ,"caret", "tidyverse", "dplyr", "naniar", "mice", "VIM", "gmodels", "ROSE")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```

## Step 1 - Collecting data

```{r}
data<- read.csv("data.Li21.csv", sep=";")
```

```{r, echo=FALSE}
#1
data<-data[complete.cases(data$outcome), ]
data<-data[,3:51]
#2
m.a.p<-(data$systolic.blood.pressure+2*data$diastolic.blood.pressure)/3
data<-cbind(data, m.a.p)
data<-data[,-c(15:16)]
#3
data$outcome<-factor(data$outcome, labels=c("Survivor", "Non-survivor"))
data$gender<-factor(data$gender, labels=c("M", "F"))
data$hypertensive<-factor(data$hypertensive, labels=c("No", "Yes"))
data$atrialfibrillation<-factor(data$atrialfibrillation, labels=c("No", "Yes"))
data$renal.failure<-factor(data$renal.failure, labels=c("No", "Yes"))
data$hyperlipemia<-factor(data$hyperlipemia, labels=c("No", "Yes"))
data$diabetes<-factor(data$diabetes, labels=c("No", "Yes"))
data$depression<-factor(data$depression, labels=c("No", "Yes"))
data$deficiencyanemias<-factor(data$deficiencyanemias, labels=c("No", "Yes"))
data$COPD<-factor(data$COPD, labels=c("No", "Yes"))
data$CHD.with.no.MI<-factor(data$CHD.with.no.MI, labels=c("No", "Yes"))
```

## Step 2 - Data clean-up and data curation

After the study of the missing values, five datasets will be created.

## Dataset A

```{r}
dataA<-data
dim(dataA) #Dimensions of the original dataset
round(mean(is.na(dataA))*100, 2)#% of the missing values
table(dataA$outcome)
```

The original dataset contains 1176 observations, 48 variables and 3,4% of missing values. 

## Dataset B = listwise deletion

This method creates a subset with the complete observations.
```{r}
sum(complete.cases(dataA))#Determine the complete observations
nrow(dataA)-sum(complete.cases(dataA))#Determine the observations with missing values
dataB<-na.omit(dataA)#Create the dataset omitting the missing values 
sum(is.na(dataB))#Checking for missing values
dim(dataB)# Dimensions of the new dataset
table(dataB$outcome)
```
The dataset B after appying the listwise deletion consist in 428 complete observations and 48 variables.

## Dataset C = deletion of variables with \>15% NA

This method is another kind of deletion method where the features with \>15% of missing values are deleted.
```{r}
dataC<- dataA[, which((apply(is.na(dataA), 2, mean)*100)<15)]
round(mean(is.na(dataC))*100, 2)#Checking for missing values
dim(dataC)# Dimensions of the new dataset
table(dataC$outcome)
```

This second method creates a data set with 1177 observations, 43 variables and an 1,22% of missing values. The omitted variables are PCO2, PH, Basophils, Lactic.acid and BMI.

## Dataset D = KNN imputations

This third method is a type of imputation method of handling missing values. It consists in a machine learning-based method that uses a Euclidean distance to find the nearest neighbors. 
```{r}
k<- round(sqrt(nrow(dataA))) #Determine the best k
dataD<-kNN(dataA, variable = colnames(dataA), k = 34, imp_var = FALSE) #Generate the imputation with k=34
round(mean(is.na(dataD)*100), 2)#Checking for missing values
dim(dataD)# Dimensions of the new dataset
table(dataD$outcome)
```
This imputation method creates a data set with the same dimensions as dataset A but without missing values (1176 observations and 48 variables). 

## Dataset E = multiple imputation

Another imputation method which consist in generating  multiple imputed values from the observed data.
```{r, warning=FALSE}
columns<- c("PCO2", "pH", "basophils", "lactic.acid", "BMI", "creatine.kinase", "lymphocyte", "neutrophils", "urine.output", "PT", "INR", "temperature", "glucose", 'm.a.p', "heart.rate", "respiratory.rate", "SP.O2", "blood.calcium") #Create a vector with the variables that need to be imputated.
pmm.data<- mice(dataA[,names(dataA) %in% columns], seed=12345, printFlag = FALSE, m = 30)#Generate the imputation
imputed.data<- mice::complete(pmm.data)
complete.data<-dataA[, which((apply(is.na(dataA), 2, mean)*100)<0.01)]
dataE<-cbind(complete.data, imputed.data)#Create the new dataset
round(mean(is.na(dataE)*100), 2)#Checking for missing values
dim(dataE)# Dimensions of the new dataset
table(dataE$outcome)
```

The multiple imputation method also creates a data set with the same dimensions as dataset A but without missing values (1176 observations and 48 variables). 

### Data training/test partition into 70-30%

```{r}
#n_train <- 0.7
#Data A
set.seed(12345)
nA<-nrow(dataA)
trainA<-sample(nA,floor(nA*params$p.train))
dataA_train<-dataA[trainA,]
dataA_test<-dataA[-trainA,]
```

```{r}
#Data B
set.seed(12345)
nB<-nrow(dataA)
trainB<-sample(nB,floor(nB*params$p.train))
dataB_train<-dataB[trainB,]
dataB_test<-dataB[-trainB,]
```

```{r}
#Data C
nC<-nrow(dataC)
trainC<-sample(nC,floor(nC*params$p.train))
dataC_train<-dataC[trainC,]
dataC_test<-dataC[-trainC,]
```

```{r}
#Data D
set.seed(12345)
nD<-nrow(dataD)
trainD<-sample(nD,floor(nD*params$p.train))
dataD_train<-dataD[trainD,]
dataD_test<-dataD[-trainD,]
```

```{r}
#Data E
set.seed(12345)
nE<-nrow(dataE)
trainE<-sample(nE,floor(nE*params$p.train))
dataE_train<-dataE[trainE,]
dataE_test<-dataE[-trainE,]
```

## Step 3 - Model training 

We are going to generate SVM models with the training data to predict the outcome. We use the ksvm() function of the kernlab package. We are going to try four different kernels: linear, Gaussian, Laplacian and hyperbolic.

### Linear

```{r,warning=FALSE,message=FALSE}
invisible(capture.output(modelA.svm.lin<- ksvm(outcome ~ ., data = dataA_train, kernel = "vanilladot")))
invisible(capture.output(modelB.svm.lin<- ksvm(outcome ~ ., data = dataB_train, kernel = "vanilladot")))
invisible(capture.output(modelC.svm.lin<- ksvm(outcome ~ ., data = dataC_train, kernel = "vanilladot")))
invisible(capture.output(modelD.svm.lin<- ksvm(outcome ~ ., data = dataD_train, kernel = "vanilladot")))
invisible(capture.output(modelE.svm.lin<- ksvm(outcome ~ ., data = dataE_train, kernel = "vanilladot")))
```

### Radial basis 

```{r,warning=FALSE,message=FALSE}
modelA.svm.rad<- ksvm(outcome ~ ., data = dataA_train, kernel = "rbfdot")
modelB.svm.rad<- ksvm(outcome ~ ., data = dataB_train, kernel = "rbfdot")
modelC.svm.rad<- ksvm(outcome ~ ., data = dataC_train, kernel = "rbfdot")
modelD.svm.rad<- ksvm(outcome ~ ., data = dataD_train, kernel = "rbfdot")
modelE.svm.rad<- ksvm(outcome ~ ., data = dataE_train, kernel = "rbfdot")
```

### Polynomial

```{r,warning=FALSE,message=FALSE}
invisible(capture.output(modelA.svm.pol<- ksvm(outcome ~ ., data = dataA_train, kernel = "polydot")))
invisible(capture.output(modelB.svm.pol<- ksvm(outcome ~ ., data = dataB_train, kernel = "polydot")))
invisible(capture.output(modelC.svm.pol<- ksvm(outcome ~ ., data = dataC_train, kernel = "polydot")))
invisible(capture.output(modelD.svm.pol<- ksvm(outcome ~ ., data = dataD_train, kernel = "polydot")))
invisible(capture.output(modelE.svm.pol<- ksvm(outcome ~ ., data = dataE_train, kernel = "polydot")))
```

### Laplacian

```{r,warning=FALSE,message=FALSE}
invisible(capture.output(modelA.svm.lap<- ksvm(outcome ~ ., data = dataA_train, kernel = "laplacedot")))
invisible(capture.output(modelB.svm.lap<- ksvm(outcome ~ ., data = dataB_train, kernel = "laplacedot")))
invisible(capture.output(modelC.svm.lap<- ksvm(outcome ~ ., data = dataC_train, kernel = "laplacedot")))
invisible(capture.output(modelD.svm.lap<- ksvm(outcome ~ ., data = dataD_train, kernel = "laplacedot")))
invisible(capture.output(modelE.svm.lap<- ksvm(outcome ~ ., data = dataE_train, kernel = "laplacedot")))
```

## Step 4 - Model prediction and evaluation

**Prediction**

We use the function `predict` to generate predictions on the testing dataset:


### Linear 

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
predict.modelA.lin <- predict(modelA.svm.lin, dataA_test)
predict.modelB.lin <- predict(modelB.svm.lin, dataB_test)
predict.modelC.lin <- predict(modelC.svm.lin, dataC_test)
predict.modelD.lin <- predict(modelD.svm.lin, dataD_test)
predict.modelE.lin <- predict(modelE.svm.lin, dataE_test)
```


### Radial basis

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
predict.modelA.rad <- predict(modelA.svm.rad, dataA_test)
predict.modelB.rad <- predict(modelB.svm.rad, dataB_test)
predict.modelC.rad <- predict(modelC.svm.rad, dataC_test)
predict.modelD.rad <- predict(modelD.svm.rad, dataD_test)
predict.modelE.rad <- predict(modelE.svm.rad, dataE_test)
```


### Polynomial

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
predict.modelA.pol <- predict(modelA.svm.pol, dataA_test)
predict.modelB.pol <- predict(modelB.svm.pol, dataB_test)
predict.modelC.pol <- predict(modelC.svm.pol, dataC_test)
predict.modelD.pol <- predict(modelD.svm.pol, dataD_test)
predict.modelE.pol <- predict(modelE.svm.pol, dataE_test)
```


### Laplacian


```{r,warning=FALSE,message=FALSE,tidy=TRUE}
predict.modelA.lap <- predict(modelA.svm.lap, dataA_test)
predict.modelB.lap <- predict(modelB.svm.lap, dataB_test)
predict.modelC.lap <- predict(modelC.svm.lap, dataC_test)
predict.modelD.lap <- predict(modelD.svm.lap, dataD_test)
predict.modelE.lap <- predict(modelE.svm.lap, dataE_test)
```


**Evaluation**

Evaluation of the classification models using a confusion matrix that compares the actual target values with those predicted by the machine learning model. 

### Linear 

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabA.svm.lin<-table(predict.modelA.lin, dataA_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabB.svm.lin<-table(predict.modelB.lin, dataB_test$outcome)
(cmatrix<-confusionMatrix(tabB.svm.lin, positive = "Non-survivor"))

roc.curve(dataB_test$outcome, predict.modelB.lin)

```


```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabC.svm.lin<-table(predict.modelC.lin, dataC_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabD.svm.lin<-table(predict.modelD.lin, dataD_test$outcome)
(cmatrix<-confusionMatrix(tabD.svm.lin, positive = "Non-survivor"))

roc.curve(dataD_test$outcome, predict.modelD.lin)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabE.svm.lin<-table(predict.modelE.lin, dataE_test$outcome)
(cmatrix<-confusionMatrix(tabE.svm.lin, positive = "Non-survivor"))

roc.curve(dataE_test$outcome, predict.modelE.lin)
```

### Radial basis

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabA.svm.rad<-table(predict.modelA.rad, dataA_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabB.svm.rad<-table(predict.modelB.rad, dataB_test$outcome)
(cmatrix<-confusionMatrix(tabB.svm.rad, positive = "Non-survivor"))

roc.curve(dataB_test$outcome, predict.modelB.rad)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabC.svm.rad<-table(predict.modelC.rad, dataC_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabD.svm.rad<-table(predict.modelD.rad, dataD_test$outcome)
(cmatrix<-confusionMatrix(tabD.svm.rad, positive = "Non-survivor"))

roc.curve(dataD_test$outcome, predict.modelD.rad)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabE.svm.rad<-table(predict.modelE.rad, dataE_test$outcome)
(cmatrix<-confusionMatrix(tabE.svm.rad, positive = "Non-survivor"))

roc.curve(dataE_test$outcome, predict.modelE.rad)
```

### Polynomial

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabA.svm.pol<-table(predict.modelA.pol, dataA_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabB.svm.pol<-table(predict.modelB.pol, dataB_test$outcome)
(cmatrix<-confusionMatrix(tabB.svm.pol, positive = "Non-survivor"))

roc.curve(dataB_test$outcome, predict.modelB.pol)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabC.svm.pol<-table(predict.modelC.pol, dataC_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabD.svm.pol<-table(predict.modelD.pol, dataD_test$outcome)
(cmatrix<-confusionMatrix(tabD.svm.pol, positive = "Non-survivor"))

roc.curve(dataD_test$outcome, predict.modelD.pol)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabE.svm.pol<-table(predict.modelE.pol, dataE_test$outcome)
(cmatrix<-confusionMatrix(tabE.svm.pol, positive = "Non-survivor"))

roc.curve(dataE_test$outcome, predict.modelE.pol)
```

### Laplacian
```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabA.svm.lap<-table(predict.modelA.lap, dataA_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabB.svm.lap<-table(predict.modelB.lap, dataB_test$outcome)
(cmatrix<-confusionMatrix(tabB.svm.lap, positive = "Non-survivor"))

roc.curve(dataB_test$outcome, predict.modelB.lap)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabC.svm.lap<-table(predict.modelC.lap, dataC_test$outcome)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabD.svm.lap<-table(predict.modelD.lap, dataD_test$outcome)
(cmatrix<-confusionMatrix(tabD.svm.lap, positive = "Non-survivor"))

roc.curve(dataD_test$outcome, predict.modelD.lap)
```

```{r,warning=FALSE,message=FALSE,tidy=TRUE}
tabE.svm.lap<-table(predict.modelE.lap, dataE_test$outcome)
(cmatrix<-confusionMatrix(tabE.svm.lap, positive = "Non-survivor"))

roc.curve(dataE_test$outcome, predict.modelE.lap)
```

Of the tested models, the linear SVMs with datasets D and E are the best. The prediction of linear and polynomial models is identical, so the simplest of the two algorithms (the linear) is selected. The difference between the predictions with datasets D and E is practically negligible; therefore, an attempt to improve them both will be made.
The models with the datasets A and C did not converge because the datasets contain missing values. 

## Step 5 - Improving model performance

To improve model's performance a k-fold cross-validation method will be used. It consist in a resampling method that uses different portions of the data to test and train a model on different iterations. 

This method will be performed on SVM linear of dataset D and E:

### Dataset D 

```{r}
set.seed(params$seed.improv)
model.kfold.D <- train(form = outcome~ . , data = dataD_train, method='svmLinear',trControl= trainControl(method='cv'), 
                  tuneGrid=NULL, trace = FALSE)
model.kfold.D
```

```{r}
predict.kfold.D<-predict(model.kfold.D, dataD_test)
resD<-table(predict.kfold.D, dataD_test$outcome)
confusionMatrix(resD, positive= "Non-survivor")

roc.curve(dataD_test$outcome, predict.kfold.D)
```

### Dataset E 

```{r}
set.seed(params$seed.improv)
model.kfold.E <- train(form = outcome~ . , data = dataE_train, method='svmLinear',trControl= trainControl(method='cv'), 
                  tuneGrid=NULL, trace = FALSE)
model.kfold.E
```

```{r}
predict.kfold.E<-predict(model.kfold.E, dataE_test)
resE<-table(predict.kfold.E, dataE_test$outcome)
confusionMatrix(resE, positive= "Non-survivor")

roc.curve(dataE_test$outcome, predict.kfold.E)
```

The results obtained are exactly the same. Therefore, in this case, the method of k-fold cross-validation did not improve our model's performance. 

