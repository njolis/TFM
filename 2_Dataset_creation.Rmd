---
title: 'Data clean-up and data curation'
author: NÃºria Jolis Orriols
date: '`r format(Sys.Date(),"%e de %B, %Y")`' 
output:
  pdf_document:
    toc: yes
    df_print: kable
---

```{r packages, message=FALSE, warning=FALSE}
libraries <- c("readr", "tidyverse", "naniar" ,"ggplot2", "car", "dplyr", "reshape2", "patchwork", "rstatix", "gridExtra", "mice", "VIM")
check.libraries <- is.element(libraries, installed.packages()[, 1])==FALSE
libraries.to.install <- libraries[check.libraries]
if (length(libraries.to.install!=0)) {
  install.packages(libraries.to.install)
}

success <- sapply(libraries,require, quietly = FALSE,  character.only = TRUE)
if(length(success) != length(libraries)) {stop("A package failed to return a success in require() function.")}
```

# Step 1 - Obtaining the data

```{r}
data<- read.csv("data/data.Li21.csv", sep=";")
```

# Step 2 - Data clean-up and data curation

The dataset consists of 1176 observations and 51 variables.

## Dataset modifications: 

1.  The first two variables are to be discarded. The variable, "group", is removed as it was created by Li 2021 to separate the data for training and testing their models, and the ID is the patient's identification which will not be useful to predict the outcome.
2. A new variable is created to combine systolic and diastolic blood pressure. It is called mean arterial pressure (MAP) and it follows the next equation: 
      MAP = [Systolic + 2*Diastolic]/3. 
3. 11 of the variables are binary, they will be converted into factors: "outcome", "gender", "hypertensive", "atrialfibrillation", "CHD.with.no.MI", "diabetes", "deficiencyanemias", "depression", "hyperlipemia", "renal.failure" and "COPD". 

```{r, echo=FALSE}
#1
data<-data[complete.cases(data$outcome), ]
data<-data[,3:51]
#2
m.a.p<-(data$systolic.blood.pressure+2*data$diastolic.blood.pressure)/3
data<-cbind(data, m.a.p)
data<-data[,-c(15:16)]
#3
data$outcome<-factor(data$outcome, labels=c("Survivor", "Non-survivor"))
data$gender<-factor(data$gender, labels=c("M", "F"))
data$hypertensive<-factor(data$hypertensive, labels=c("No", "Yes"))
data$atrialfibrillation<-factor(data$atrialfibrillation, labels=c("No", "Yes"))
data$renal.failure<-factor(data$renal.failure, labels=c("No", "Yes"))
data$hyperlipemia<-factor(data$hyperlipemia, labels=c("No", "Yes"))
data$diabetes<-factor(data$diabetes, labels=c("No", "Yes"))
data$depression<-factor(data$depression, labels=c("No", "Yes"))
data$deficiencyanemias<-factor(data$deficiencyanemias, labels=c("No", "Yes"))
data$COPD<-factor(data$COPD, labels=c("No", "Yes"))
data$CHD.with.no.MI<-factor(data$CHD.with.no.MI, labels=c("No", "Yes"))
str(data)
```
Finally, we got a dataset of 1176 observations and 48 variables being 39 numeric and 11 factors. Outcome is the response variable whose behavior shall be modeled and the 49 variables left are considered to be candidate predictors.

## Checking for missing values

```{r}
#Determine the total number of missing values
sum(is.na(data))
#In percentage and graphic visualization
round(mean(is.na(data))*100, 1)
vis_miss(data)
#Variables with % of missing values
x1<-apply(is.na(data), 2, mean)
x2<-round(x1[x1>0], 3)*100 #%NA per variable
x2
```

The 3.4% of the values are missing and are concentrated in 21 of the 50 variables. Of those 21 variables, 8 of them present more than 10% of missing values: "basophils", "creatine.kinase", "lactic.acid", "BMI", "neutrophils", "lymphocyte", "pH" and "PCO2".


```{r, warning=FALSE}
#To analyze the patron of the missing values we create a dataset with those variables and then we plot them: 
var.na<-dplyr::select(data, c("PCO2", "pH", "basophils", "lactic.acid", "BMI", "creatine.kinase", "lymphocyte", "neutrophils", "urine.output", "PT", "INR", "temperature", "glucose", 'm.a.p', "heart.rate", "respiratory.rate", "SP.O2", "blood.calcium"))

aggr_plot<-aggr(var.na, col=c("navyblue", "red"), numbers=TRUE, labels=names(var.na), ylab=c("Histogram of missing values", "Pattern of missing values"), prop = c(TRUE, FALSE), cex.axis=.4)

```

```{r}
ggsave(filename="results_missingvalues_pattern.png", path="~/Desktop/UOC/TFM/R/Figures", width = 5, height = 4, dpi = 150, units = "in", device='png')
```

The histogram on the left side shows the proportion of missing values in each variables. The graphic on the right side shows the pattern of missing values, in navy blue the observed values and in red color the missing values. It seems that some of the features have a pattern of missing data (there are several red cells in the same row). Because of that, MCAR gets discarded. 

The prediction models are very sensitive to missing values, so we would have to take measures in order to make predictions.

In order to study the models performance we will create four different datasets: 

## Dataset A = data with no methodologies applied

```{r}
dataA<-data
dim(dataA) #Dimensions of the original dataset
round(mean(is.na(dataA))*100, 2)#% of the missing values
table(dataA$outcome)
```

The original dataset contains 1176 observations, 48 variables and 3,4% of missing values. 

## Dataset B = listwise deletion

This method creates a subset with the complete observations.
```{r}
sum(complete.cases(dataA))#Determine the complete observations
nrow(dataA)-sum(complete.cases(dataA))#Determine the observations with missing values
dataB<-na.omit(dataA)#Create the dataset omitting the missing values 
sum(is.na(dataB))#Checking for missing values
dim(dataB)# Dimensions of the new dataset
table(dataB$outcome)
```
The dataset B after appying the listwise deletion consist in 428 complete observations and 48 variables.

## Dataset C = KNN imputation

This third method is a type of imputation method of handling missing values. It consists in a machine learning-based method that uses a Euclidean distance to find the nearest neighbors. 
```{r}
k<- round(sqrt(nrow(dataA))) #Determine the best k
dataC<-kNN(dataA, variable = colnames(dataA), k = 34, imp_var = FALSE) #Generate the imputation with k=34
round(mean(is.na(dataC)*100), 2)#Checking for missing values
dim(dataC)# Dimensions of the new dataset
table(dataC$outcome)
```
This imputation method creates a data set with the same dimensions as dataset A but without missing values (1176 observations and 48 variables). 

## Dataset D = MICE

Another imputation method which consist in generating  multiple imputed values from the observed data.
```{r, warning=FALSE}
columns<- c("PCO2", "pH", "basophils", "lactic.acid", "BMI", "creatine.kinase", "lymphocyte", "neutrophils", "urine.output", "PT", "INR", "temperature", "glucose", 'm.a.p', "heart.rate", "respiratory.rate", "SP.O2", "blood.calcium") #Create a vector with the variables that need to be imputated.
pmm.data<- mice(dataA[,names(dataA) %in% columns], seed=12345, printFlag = FALSE, m = 30, method = "pmm")#Generate the imputation
imputed.data<- mice::complete(pmm.data)
complete.data<-dataA[, which((apply(is.na(dataA), 2, mean)*100)<0.01)]
dataD<-cbind(complete.data, imputed.data)#Create the new dataset
round(mean(is.na(dataD)*100), 2)#Checking for missing values
dim(dataD)# Dimensions of the new dataset
table(dataD$outcome)
```

The multiple imputation method also creates a data set with the same dimensions as dataset A but without missing values (1176 observations and 48 variables). 

```{r}
write.csv(dataA,"~/Desktop/UOC/TFM/R/TFM/TFM_UOC/data/datasetA.csv", row.names = FALSE)
write.csv(dataB,"~/Desktop/UOC/TFM/R/TFM/TFM_UOC/data/datasetB.csv", row.names = FALSE)
write.csv(dataC,"~/Desktop/UOC/TFM/R/TFM/TFM_UOC/data/datasetC.csv", row.names = FALSE)
write.csv(dataD,"~/Desktop/UOC/TFM/R/TFM/TFM_UOC/data/datasetD.csv", row.names = FALSE)
```

